---
title: "Using cTWAS with summary statistics"
author: "Kaixuan Luo, Sheng Qian"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using multigroup cTWAS with summary statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      fig.width = 6,
                      fig.height = 4,
                      fig.align = "center",
                      fig.cap = "&nbsp;",
                      dpi = 120)
```


## Overview

This document demonstrates how to use the summary statistics version of cTWAS. Running cTWAS involves the main steps: preparing input data, computing gene z-scores, assembling the input data for all regions, estimating parameters, screening regions with strong signals, and fine-mapping the genes and variants. 

The output of cTWAS are posterior inclusion probabilities (PIPs) for all variants and molecular traits. This document will cover each of these topics in detail. We also describe some of the options available at each step and the analysis considerations behind these options. 

This document shows the updated pipeline for running cTWAS with summary statistics, 
which extends our earlier cTWAS method (single group version) to integrate multiple groups of prediction models,  e.g. multiple tissues, cell types or conditions, or multiple molecular traits.

We still keep the cTWAS single group version [here]() in case someone needs to reproduce the results from our paper. 



## Installation

Install the `multigroup` branch of `cTWAS` package

```{r install_package, eval=FALSE}
remotes::install_github("xinhe-lab/ctwas", ref = "multigroup_test")
```


Load the package
```{r load_package}
library(ctwas)
```


## Preparing input data

In this version of `cTWAS` package, we require first running the data preprocessing and harmonization steps 
before running parameter estimation and finemapping steps of cTWAS analysis. 

The inputs for the summary statistics version of cTWAS include GWAS summary statistics, 
prediction models, and LD reference. 

We used the directories and files on the University of Chicago RCC cluster as examples. If you are at UChicago, you can load those data from RCC as below.

### GWAS z-scores

We read the GWAS summary statistics as a data frame `z_snp`, with columns "id", "A1", "A2", "z", and each row is a variant. 
`A1` is the alternate allele, and `A2` is the reference allele. 

For this example, we will use summary statistics from a GWAS of LDL cholesterol in the UK Biobank. We will download the VCF from the IEU Open GWAS Project. 
```{r, eval=FALSE}
# download the summary statistics, and unzip the file.
dir.create("gwas_summary_stats")
system("wget https://gwas.mrcieu.ac.uk/files/ukb-d-30780_irnt/ukb-d-30780_irnt.vcf.gz -P gwas_summary_stats")
#R.utils::gunzip("gwas_summary_stats/ukb-d-30780_irnt.vcf.gz")
```


Next, we will read the summary statistics. Then, we will compute the z-scores and format the input data. We will also collect the sample size, which will be useful later. We will save this output for convenience.

```{r, eval=FALSE}
# set output directory and output name

outputdir <- "/project2/xinhe/shared_data/multigroup_ctwas/LDL_multigroup_tutorial/output"
dir.create(outputdir, showWarnings=F, recursive=T)

outname <- "LDL_Multiomics"
gwas_name <- "ukb-d-30780_irnt"
```


```{r, eval=FALSE}
# read the data using the VariantAnnotation package
z_snp <- VariantAnnotation::readVcf("gwas_summary_stats/ukb-d-30780_irnt.vcf.gz")
z_snp <- as.data.frame(gwasvcf::vcf_to_tibble(z_snp))

# compute the z-scores
z_snp$Z <- z_snp$ES/z_snp$SE

# collect sample size (most frequent sample size for all variants)
gwas_n <- as.numeric(names(sort(table(z_snp$SS),decreasing=TRUE)[1]))
cat("gwas_n=", gwas_n, "\n")

# subset the columns and format the column names
z_snp <- z_snp[,c("rsid", "ALT", "REF", "Z")]
colnames(z_snp) <- c("id", "A1", "A2", "z")

z_snp_outfile <- file.path(outputdir, paste0(gwas_name, ".z_snp.RDS"))
gwas_n_outfile <- file.path(outputdir, paste0(gwas_name, ".gwas_n.RDS"))
saveRDS(z_snp, file=z_snp_outfile)
saveRDS(gwas_n, file=gwas_n_outfile)

```


If your GWAS summary statistics is of other formats, please extract relevant columns (rsid, alternative allele, reference allele, zscore) and convert it to the format shown above. 

### Prediction models

Prediction models can be specified in either  PredictDB or FUSION format. 
PredictDB is the recommended format, as it has information about correlations among SNPs included in the model. This is useful for recovering strand ambiguous variants. In this user guide, we focus on the PredictDB format, but will provide information on using FUSION format.

In terms of the choice of prediction models, cTWAS performs best when prediction models are sparse, i.e. they have relatively few variants per gene. As the density of variants increases, it becomes computationally more expensive. Dense variants may also lead to a problem with region merging. Basically, if the variants in the prediction model of a gene spans two LD-independent regions, it would be unclear to cTWAS what region the gene should be assigned to. So cTWAS will attempt to merge the two regions. But if many genes have dense variants in their prediction models, region merging could be excessive, leading to very large regions and hurting the performance of cTWAS. Given this consideration, we recommend choosing sparse prediction models such as Lasso. If using dense prediction models, we recommend removing variants with weights below a threshold from the prediction models.

Often, a researcher may perform eQTL mapping and have a list of significant eQTLs without explicitly building prediction models. In such cases, it is possible to run cTWAS. This can be done simply by using top eQTL per gene as the prediction model. One can create PredictDB format data from the eQTL list - the details will be added later. 

Please check [PredictDB](http://predictdb.org/) for the format of PredictDB weights. To specify weights in PredictDB format, provide the path to the `.db` file. 

Please check [Fusion/TWAS](http://gusevlab.org/projects/fusion/#compute-your-own-predictive-models) for the format of FUSION weights. To specify weights in FUSION format, provide the path to the folder that contains the `.wgt.RDat` files. The `.pos` file which points to the individual *.RDat weight files are assumed to be outside the folder. For more format details, check the FUSION website. 

For this analysis, we will use liver and subcutaneous adipose gene expression models trained on GTEx v8 in the PredictDB format. We will download both the prediction models (.db) and the covariances between variants in the prediction models (.txt.gz). The covariances can optionally be used for computing LD.
```{r, eval=FALSE}
# download the files
system("wget https://zenodo.org/record/3518299/files/mashr_eqtl.tar")

# extract to ./weights folder 
system("mkdir /project2/xinhe/shared_data/ctwas_tutorial/weights")
system("tar -xvf mashr_eqtl.tar -C /project2/xinhe/shared_data/ctwas_tutorial/weights")
system("rm mashr_eqtl.tar")
```


### LD reference and region info

LD reference information can be provided as genetic correlation matrices (termed "R matrices") 
for regions that are approximately LD-independent. 

cTWAS performs its analysis region-by-region. The preferred way to run cTWAS is to provide pre-computed LD matrices for each region. 

It is critical that the genome build (e.g. hg38) of the LD reference matches the genome build used to train the prediction models. The genome build of the GWAS summary statistics does not matter because variant positions are determined by the LD reference.

The choice of LD reference population is important for fine-mapping. Best practice for fine-mapping is to use an in-sample LD reference (LD computed using the subjects in the GWAS sample). If in-sample LD reference is not an option, the LD reference should be as representative of the population in the GWAS sample as possible. Given that cTWAS is an extended fine-mapping algorithm, and that gene z-scores are computed using the observed GWAS z-scores, which reflect patterns of LD in the GWAS population, our recommendation is to match the LD reference to the GWAS population, not the population used to build the prediction models. 

#### Defining regions

cTWAS includes predefined regions based on European (EUR), Asian (ASN), or African (AFR) populations, using either genome build b38 or b37. 
These regions were previously generated using [LDetect](https://github.com/endrebak/ldetect). 

#### LD matrices

To use LD matrices for the LD reference, provide a directory containing all of the `.RDS` matrix files and matching `.Rvar` variant information files. 

We have precomputed reference LD matrices and the variant information tables accompanying the LD matrices for both UKB and 1000G European Phase3 references.
The complete LD matrices of European individuals from UK Biobank can be downloaded [here](https://uchicago.box.com/s/jqocacd2fulskmhoqnasrknbt59x3xkn). On the University of Chicago RCC cluster, the b38 reference is available at `/project2/mstephens/wcrouse/UKB_LDR_0.1/` and the b37 reference is available at `/project2/mstephens/wcrouse/UKB_LDR_0.1_b37/`.

#### Region info

In this version of the package, we also require a data frame `region_info` containing the following columns: 
"chrom", "start", "stop", "region_id", "LD_matrix", "SNP_info".

"chrom", "start", "stop" are the genomic coordinates of the regions, 
"region_id" contains the IDs of the region, and we use "chr:start-stop" format in region IDs by default.
"LD_matrix" stores the paths to the precomputed LD (R) matrices (`.RDS` files in our precomputed reference LD files),
"SNP_info" stores the paths to the variant information corresponding to the LD matrices (`.Rvar` files in our precomputed reference LD files). 

The `.RDS` file is [R .RDS format](https://www.rdocumentation.org/packages/base/versions/3.3.2/topics/readRDS?tap_a=5644-dce66f&tap_s=10907-287229). It stores the LD correlation matrix for a region (a $p \times p$ matrix, $p$ is the number of variants in the region). We require that for each `.RDS` file, in the same directory, there is a corresponding file with the same stem but ending with the suffix `.Rvar`. This `.Rvar` files includes variant information for the region, and the order of its rows must match the order of rows and columns in the `.RDS` file.

Here we use the b38 European region file, which is included in the package.

```{r region_info, eval=FALSE}
genome_version <- "b38"
ld_R_dir <- "/project2/mstephens/wcrouse/UKB_LDR_0.1/"

region_file <- system.file("extdata/ldetect", paste0("EUR.", genome_version, ".bed"), package = "ctwas")
region_info <- read.table(region_file, header = TRUE)
colnames(region_info)[1:3] <- c("chrom", "start", "stop")
region_info$chrom <- as.numeric(gsub("chr", "", region_info$chrom))
region_info$region_id <- paste0(region_info$chr, ":", region_info$start, "-", region_info$stop)

filestem <- paste0("ukb_", genome_version, "_0.1")
ld_filestem <- sprintf("%s_chr%s.R_snp.%s_%s", filestem, region_info$chrom, region_info$start, region_info$stop)
region_info$LD_matrix <- file.path(ld_R_dir, paste0(ld_filestem, ".RDS"))
region_info$SNP_info <- file.path(ld_R_dir, paste0(ld_filestem, ".Rvar"))
# Check to make sure all the LD matrices and SNP info files are available.
stopifnot(all(file.exists(region_info$LD_matrix)))
stopifnot(all(file.exists(region_info$SNP_info)))
saveRDS(region_info, file.path(outputdir, "region_info.RDS"))
```


The columns of the `.Rvar` file include information on chromosome, variant name, position in base pairs, and the alternative and reference alleles. The `variance` column is the variance of each variant prior to standardization; this is required for PredictDB weights but not FUSION weights. PredictDB weights should be scaled by the variance before imputing gene expression. This is because PredictDB weights assume that variant genotypes are not standardized before imputation, but our implementation assumes standardized variant genotypes. If variance information is missing, or if weights are in PredictDB format but are already on the standardized scale (e.g. if they were converted from FUSION to PredictDB format), this scaling can be turned off using the option `scale_by_ld_variance=FALSE` using the multigroup version of cTWAS. We've also included information on allele frequency in the variant info, but this is optional.

The naming convention for the LD matrices is `[filestem]_chr[#].R_snp.[start]_[end].RDS`. cTWAS expects that all `.RDS` and `.Rvar` files in the directory contain LD information, so no other files with these suffixes should be in the directory. Each variant should be uniquely assigned to a region, and the regions should be left closed and right open, i.e. [start, stop). The positions of the LD matrices must match exactly the positions specified by the region file. Do not include invariant or multiallelic variants in the LD reference. 

#### Convert genotype data to LD matrices

We could use the `convert_geno_to_LD_matrix` function to convert genotype files (and LD regions) to the LD matrices and corresponding variant information. 

Below is an example of generating LD matrices in hg38 using UKB genotype data.

```{r convert_geno_to_LD_matrix, eval=FALSE}
# specify LD reference
ldref_dir <- "/gpfs/data/xhe-lab/ukb_LDR/genotype_data_0.1"
genotype_files <- file.path(ldref_dir, paste0("ukb_chr", 1:22, ".pgen"))
# the output Rvar files use the positions and allele information in varinfo_files
varinfo_files <- "/gpfs/data/xhe-lab/ukb_LDR/neale_lab/neale_variants_hg38.bim"
# prepare a data frame region_info for LD regions with columns "chr", "start", and "stop"
# the positions should match those in varinfo_files
region_file <- system.file("extdata/ldetect", "EUR.b38.bed", package = "ctwas")
region_info <- read.table(region_file, header = TRUE, stringsAsFactors = FALSE)
# specify output
outputdir <- "/gpfs/data/xhe-lab/ctwas/LDR/UKB_b38/"
outname <- "ukb_b38_0.1"

# generate LD matrices (.RDS) and variant info (.Rvar)
# update region info with paths of LD matrices and variant info files
# we could run this for one chromosome or multiple chromosomes
updated_region_info <- convert_geno_to_LD_matrix(region_info, 
                                                 genotype_files, 
                                                 varinfo_files,
                                                 chrom = 1:22,
                                                 outputdir = outputdir, 
                                                 outname = outname)
```


We provide example R scripts to generate the LD matrices for UKB or 1KG European genotype files [here](https://github.com/xinhe-lab/ctwas/tree/multigroup_test/inst/extdata/scripts)

### Data harmonization

There are a few potential problems when preparing the input data. First, the variants in the three sets of input data may not match. Only variants in all three input data will be used in cTWAS. So it is important to maximize the overlap of the variants in the three sets. This can be done for example, by imputing GWAS summary statistics of the variants missing in GWAS but in the LD reference. Another useful pre-processing step is to perform Minor allele frequency (MAF) filtering on the GWAS data so that only those with MAF above a certain cutoff would be used in the analysis, ideally the same cutoff used in the LD references. Additionally, when building the prediction models of gene expression, it is better to impute the genotype data using the LD reference, if possible.  All these steps should be done before running cTWAS.  

The second potential problem is that the effect alleles in the prediction model, GWAS and LD reference may not agree with each other, thus we need to ``harmonize'' the data to ensure that the effect alleles match. If the data are not already harmonized, we provide some options for harmonization. 

These inputs should be harmonized prior to cTWAS analysis (i.e. the reference and alternative alleles for each variant should match across all three data sources). 
    
Another potential problem is the LD of the GWAS data (in-sample LD) do not match the reference LD. This can lead to false positives in fine-mapping tools. Diagnostic tools including [SuSiE-RSS][susierss_diagnostic], and [DENTIST][DENTIST], have been developed to check possible LD mismatch. Because it is very time consuming to run the LD mismatch diagnosis for all the regions across the genome, we will perform LD mismatch diagnosis and adjustment only for selected regions with high PIP signals in the post-processing section. 


#### Harmonizing GWAS z-scores and LD reference

The `preprocess_z_snp()` function harmonizes GWAS z-scores and LD reference based on the included allele information.
If `drop_multiallelic = TRUE`, it will drop multiallelic variants. 
If `drop_strand_ambig = TRUE`, it will drop strand ambiguous variants.

```{r preprocess_z_snp, eval=FALSE}
# load the GWAS sumstats data
z_snp_outfile <- file.path(outputdir, paste0(gwas_name, ".z_snp.RDS"))
z_snp <- readRDS(z_snp_outfile)

z_snp <- preprocess_z_snp(z_snp, region_info, drop_multiallelic = TRUE, drop_strand_ambig = TRUE)
saveRDS(z_snp, file.path(outputdir, paste0(outname, ".preprocessed.z_snp.RDS")))
```


#### Harmonizing prediction models and LD reference

The `preprocess_weight()` function harmonizes the PredictDB/FUSION prediction models and LD reference. 
This only needs to be done once per combination of prediction models and LD reference. 

In this example, we use liver and subcutaneous adipose gene expression models. We preprocess each weight file separately and combine them in the end. 
We specify PredictDB or FUSION format in `weight_format` and the specific method in FUSION by `method_Fusion`. 
We specify the `type` as the molecular QTL types, and `context` as the tissues, cell types, or conditions. 
For the expression models, we limit to protein coding genes, by setting `filter_protein_coding_genes = TRUE`.
We set `scale_by_ld_variance=TRUE` for PredictDB weights because PredictDB assumes the unstandardized genotypes. 
For PredictDB models, we set `load_predictdb_LD=TRUE` to load pre-computed correlations (`.txt.gz` file) between weight variants. 
For FUSION models, we set `load_predictdb_LD=FALSE` to calculate correlations between weight variants with LD reference.

We use 6 cores to parallelize the computation over the weights.

````{r preprocess_weights, eval=FALSE}
weight_files <- c("/project2/xinhe/shared_data/multigroup_ctwas/weights/expression_models/mashr_Liver.db",       "/project2/xinhe/shared_data/multigroup_ctwas/weights/expression_models/mashr_Adipose_Subcutaneous.db")

ncore <- 6

weights_liver <- preprocess_weights(weight_file = weight_files[1],
                                    region_info,
                                    z_snp$id,
                                    type = "eQTL",
                                    context = "liver",
                                    weight_format = "PredictDB",
                                    ncore = ncore,
                                    drop_strand_ambig = TRUE,
                                    scale_by_ld_variance = TRUE,
                                    load_predictdb_LD = TRUE,
                                    filter_protein_coding_genes = TRUE)

weights_adipose <- preprocess_weights(weight_file = weight_files[2],
                                      region_info,
                                      z_snp$id,
                                      type = "sQTL",
                                      context = "liver",
                                      weight_format = "PredictDB",
                                      ncore = ncore,
                                      drop_strand_ambig = TRUE,
                                      scale_by_ld_variance = TRUE,
                                      load_predictdb_LD = TRUE,
                                      filter_protein_coding_genes = TRUE)

weights <- c(weights_liver, weights_adipose)
saveRDS(weights, file.path(outputdir, paste0(outname, ".preprocessed.weights.RDS")))
```



## Running cTWAS main function

We can run the entire process of cTWAS analysis from computing z-scores of molecular traits, 
estimating parameters to finemapping using the main `ctwas_sumstats` function.

The `ctwas_sumstats` function takes preprocessed GWAS z-scores (`z_snp`), `weights`, `region_info` as input, and will perform the steps: computing z-scores, assembling input region_data, estimating parameters, screening regions, and finemapping. It will output a list including: estimated parameters, fine-mapping results, boundary genes, and region_data.

Some main parameter settings: 
The `thin` argument randomly selects a subset of variants (10% when thin = 0.1) to use during the parameter estimation and screening regions, reducing computation. 
The `maxSNP` sets a maximum on the number of variants that can be in a single region to prevent memory issues during fine-mapping. 
The `ncore` argument specifies the number of cores to use when parallelizing over regions.
The `niter_prefit` argument and `niter` argument sets the number of EM iterations for estimating parameters. There are several options for how to handle the prior effect size parameters by specifying the `group_prior_var_stucture`. See the section "Estimating parameters" for more details about these settings. 
The `L` argument sets the number of effects for SuSiE during the screening regions and fine mapping steps.
If the `save_cor` argument is TRUE, it will save the correlation matrices (SNP-SNP, SNP-gene, and gene-gene correlation matrices) in the `cor_dir` directory for future access. 
If the `logfile` argument is specified, it will write log messages to the log file. 
If the `verbose` argument is TRUE, it will print more detailed messages. 
 
```{r ctwas_sumstats, eval=FALSE}
res <- ctwas_sumstats(z_snp, weights, region_info, 
                      thin = 0.1,
                      niter_prefit = 3, 
                      niter = 30, 
                      L = 5, 
                      group_prior_var_structure = "shared_type", 
                      maxSNP = 20000, 
                      min_nonSNP_PIP = 0.5,
                      ncore = ncore, 
                      save_cor = TRUE, 
                      cor_dir = file.path(outputdir, "cor_matrix"),
                      logfile = file.path(outputdir, 
                                paste0(outname, ".ctwas_sumstats.log")),
                      verbose = FALSE)

param <- res$param
finemap_res <- res$finemap_res
boundary_genes <- res$boundary_genes
region_data <- res$region_data
z_gene <- res$z_gene
```



## Running cTWAS steps separately 

The `ctwas_sumstats` function contains several cTWAS steps. 
We can run each of the cTWAS steps separately.
This would allow more flexible controls of the cTWAS modules with different settings 
(memory, parallelization, etc.).
We can save the results from each module for future access.


### Computing z-scores of molecular traits.

After we have done data preprocessing, we compute z-scores of molecular traits. This step basically performs Transcriptome-wide association studies (TWAS) on each gene with a prediction model. The underlying calculation is based on [S-PrediXcan][S-PrediXcan].

The `compute_gene_z` function computes z-scores for the molecular traits using preprocessed SNP z-scores (`z_snp`) and the preprocessed weights. 

*Note: the software uses the term “gene”, but they could be any molecular trait.*

```{r compute_gene_z, eval=FALSE}
z_gene <- compute_gene_z(z_snp, weights, ncore=ncore)
saveRDS(z_gene, file = file.path(outputdir, paste0(outname, ".z_gene.RDS")))
```


### Assemble input data for the regions

After we have done data preprocessing and imputed gene z-scores, we assemble the input data for all the regions.  
The `assemble_region_data` function will assign genes, SNPs and their z-score data for each region.
It will identify the genes that cross region boundaries, and reassign them to the region with the highest r2. 
It will down-sample the SNPs by the value of `thin` to reduce running time of parameter estimation and region screening. If `thin = 1`, it will use all the SNPs. 
It will trim (randomly by default) the SNPs if the number of SNPs in a region is more than `maxSNP`.

```{r assemble_region_data, eval=FALSE}
thin <- 0.1
max_snp_region <- 20000
res <- assemble_region_data(region_info, z_snp, z_gene, weights,
                            thin = thin,
                            maxSNP = max_snp_region,
                            trim_by = "random",
                            adjust_boundary_genes = TRUE,
                            ncore = ncore)
region_data <- res$region_data
boundary_genes <- res$boundary_genes

saveRDS(res, file.path(outputdir, paste0(outname, ".region_data.thin", thin, ".RDS")))
```


This functions returns a list with `region_data`, a list of regions with assembled input data, 
as well as a data frame of cross-boundary genes.

### Estimating parameters

We use the `est_param` function to estimate two sets of parameters: the prior inclusion probabilities and the prior effect size variance. It will take the assembled `region_data` as input. 

There are several options for how to handle the prior effect size parameters by specifying the `group_prior_var_stucture`:

- "shared_type" (default option) allows all groups in one molecular QTL type to share the same variance parameter.
- "shared_context" allows all groups in one molecular QTL type to share the same variance parameter.
- "shared_nonSNP" allows all non-SNP groups to share the same variance parameter.
- "shared_all" allows all groups to share the same variance parameter.
- "independent" allows all groups to have their own separate variance parameters.

This step will run the EM algorithm to estimate parameters and return estimate parameters (`group_prior`, `group_prior_var`, etc.).
It will run two rounds of EM algorithm, first round uses fewer iterations (`niter_prefit=3` by default) to get rough parameter estimates, 
and using the estimated priors in the first round, we select regions with single effect to run the second round.
We use more iterations in the second round (`niter=30` by default) to get accurate parameter estimates. 

```{r est_param, eval=FALSE}
param <- est_param(region_data, 
                   group_prior_var_structure = "shared_type", 
                   niter_prefit = 3,
                   niter = 30, 
                   ncore = ncore)
group_prior <- param$group_prior
group_prior_var <- param$group_prior_var
saveRDS(param, file.path(outputdir, paste0(outname, ".param.RDS")))
```


We could use the `summarize_param()` function to assess the convergence of the estimated parameters and to compute the proportion of variance explained (PVE) by variants and genes. 
```{r summarize_param, eval=FALSE}
ctwas_parameters <- summarize_param(param, gwas_n)
saveRDS(ctwas_parameters, file.path(outputdir, paste0(outname, ".ctwas_parameters.RDS")))
```


### Screening regions

After parameter estimation, cTWAS performs a screening process to fine-map all regions using thinned variants, and select regions with strong signals (total non-SNP PIP > 0.5 by default). It returns a data frame with region IDs and nonSNP PIPs for each region. We could further narrow down the list of regions by using more stringent `min_nonSNP_PIP` cutoffs.

```{r screen_regions, eval=FALSE}
region_nonSNP_PIP_df <- screen_regions(region_data,
                                       region_info,
                                       weights,
                                       L = 5,
                                       group_prior = group_prior,
                                       group_prior_var = group_prior_var,
                                       min_nonSNP_PIP = 0.5,
                                       ncore = ncore,
                                       verbose = TRUE)
screened_region_ids <- region_nonSNP_PIP_df$region_id
screened_region_data <- region_data[screened_region_ids]
```


After selecting regions with strong signals, we expand the screened regions with 
full sets of SNPs for final finemapping. This step is not needed if `thin = 1`.
```{r expand_region_data, eval=FALSE}
if (thin < 1){
  screened_region_data <- expand_region_data(screened_region_data,
                                             region_info,
                                             z_snp,
                                             z_gene,
                                             trim_by = "z",
                                             maxSNP = max_snp_region,
                                             ncore = ncore)

  # update data in screened regions with screened_region_data
  region_data[names(screened_region_data)] <- screened_region_data
  saveRDS(region_data, file.path(outputdir, paste0(outname, ".region_data.RDS")))
}
```


### Fine-mapping screened regions

We now perform the fine-mapping step for each of the screened regions with the estimated parameters.

The `finemap_regions` function performs fine-mapping for the screened regions in `screened_region_data`.

It first computes correlation matrices (SNP-SNP, SNP-gene, and gene-gene correlation matrices), 
and then runs fine-mapping with estimated parameters (`group_prior` and `group_prior_var`),
and returns a data frame with annotated fine-mapping results for the regions.

If `save_cor = TRUE`, it will save the computed correlation matrices to `cor_dir` for future access.

```{r finemap_regions, eval=FALSE}
finemap_res <- finemap_regions(screened_region_data,
                               region_info,
                               weights,
                               group_prior = group_prior,
                               group_prior_var = group_prior_var,
                               L = 5,
                               save_cor = TRUE,
                               cor_dir = paste0(outputdir, "/cor_matrix/"),
                               ncore = ncore,
                               verbose = TRUE)
saveRDS(finemap_res, file.path(outputdir, paste0(outname, ".ctwas.res.RDS")))
```



### Fine-mapping a single region

We could run finemapping for a single region (or multiple regions) with precomputed parameters and full sets of SNPs. 

Here we show an example for finemapping a single region "16:71020125-72901251".

Since we already assembled `region_data` for all the regions, we can expand the region of interest with full SNPs. 

```{r finemap_1_region, eval=FALSE}
region_id <- "16:71020125-72901251"
selected_region_data <- expand_region_data(region_data[region_id],
                                           region_info,
                                           z_snp,
                                           z_gene,
                                           trim_by = "z")
```


We can perform finemapping for this region of interest:
```{r finemap_1_region_3, eval=FALSE}
finemap_region_res <- finemap_regions(selected_region_data,
                                      region_info,
                                      weights,
                                      group_prior = group_prior,
                                      group_prior_var = group_prior_var,
                                      L = 5,
                                      save_cor = TRUE,
                                      cor_dir = file.path(outputdir, "cor_matrix"),
                                      verbose = TRUE)
```



## Post-processing

### Merging regions

We first select cross-boundary genes to merge, 
We then identify overlapping regions from these cross-boundary genes, 
and create `merged_region_data` and `merged_region_info` for these merged regions. 

We rerun finemapping for the merged regions, and update the finemapping results.

```{r merge_regions, eval=FALSE}
high_PIP_genes <- unique(finemap_res[finemap_res$type != "SNP" & finemap_res$susie_pip >= 0.5, "id"])
selected_boundary_genes <- boundary_genes[boundary_genes$id %in% high_PIP_genes, , drop=FALSE]

if (nrow(selected_boundary_genes) > 0){
  res <- merge_finemap_regions(selected_boundary_genes,
                               region_data,
                               region_info,
                               finemap_res,
                               z_snp,
                               z_gene,
                               weights,
                               expand = TRUE,
                               group_prior = group_prior,
                               group_prior_var = group_prior_var,
                               L = 5,
                               save_cor = TRUE,
                               cor_dir = paste0(outputdir, "/cor_matrix/"),
                               ncore = 1,
                               verbose = TRUE)
  merged_region_data <- res$merged_region_data
  merged_region_info <- res$merged_region_info
  finemap_res <- res$finemap_res
}

```


### Rerun finemapping with L = 1 for regions with LD mismatches

LD mismatches between GWAS data and the reference LD could lead to false positives in fine-mapping. Diagnostic tools including [SuSiE-RSS][susierss_diagnostic], and [DENTIST][DENTIST], have been developed to check possible LD mismatch. 

Here, we perform the LD mismatch diagnosis using [SuSiE-RSS][susierss_diagnostic] for selected regions. It is an optional step that users could run after finishing the main cTWAS analysis. Users could choose regions of interest, to be confident of the results.

Here we check the top 10 regions with highest total non-SNP PIPs:
```{r, eval=FALSE}
nonSNP_PIP_df <- compute_region_nonSNP_PIPs(finemap_res)
selected_nonSNP_PIP_df <- nonSNP_PIP_df[order(nonSNP_PIP_df$nonSNP_PIP, decreasing = TRUE),]
selected_region_ids <- selected_nonSNP_PIP_df$region_id[1:10]
loginfo("Number of selected regions: %d", length(selected_region_ids))
```


We perform LD mismatch diagnosis for these regions using SuSiE RSS to detect problematic SNPs. This returns a list of problematic SNPs, flipped SNPs, and the test statistics of `kriging_rss` function from SuSiE-RSS. 
```{r, eval=FALSE}
selected_region_info <- region_info[region_info$region_id %in% selected_region_ids, ]
# We use SuSiE RSS to perform LD mismatch diagnosis for these high PIP regions.
res <- detect_ld_mismatch_susie(z_snp, selected_region_info, gwas_n, ncore = ncore, p_diff_thresh = 5e-8)
problematic_snps <- res$problematic_snps
flipped_snps <- res$flipped_snps
condz_stats <- res$condz_stats

detect_ld_mismatch_file <- file.path(outputdir, paste0(outname, ".detect_ld_mismatch_res.RDS"))
loginfo("%d problematic SNPs", length(problematic_snps))
saveRDS(res, detect_ld_mismatch_file)
```


We get large effect genes (molecular traits) (abs(z) > 3) with problematic SNPs in their weights, and then select regions containing the problematic genes (molecular traits).
```{r}
problematic_genes <- get_problematic_genes(problematic_snps, weights, z_gene, z_thresh = 3)
loginfo("%d problematic genes", length(problematic_genes))

problematic_region_ids <- unique(finemap_res[finemap_res$id %in% problematic_genes, "region_id"])
loginfo("%d problematic regions", length(problematic_region_ids))
```

We then rerun the fine-mapping with L = 1 for the problematic regions, and update finemapping results. 
```{r, eval=FALSE}
if (length(problematic_region_ids) > 0){
  # rerun the fine-mapping with L = 1 for the problematic regions
  rerun_region_data <- screened_region_data[problematic_region_ids]
  finemap_problematic_regions_res <- finemap_regions(rerun_region_data,
                                                     region_info,
                                                     weights,
                                                     group_prior = group_prior,
                                                     group_prior_var = group_prior_var,
                                                     L = 1,
                                                     cor_dir = paste0(outputdir, "/cor_matrix/"),
                                                     ncore = ncore,
                                                     verbose = TRUE)
  # update finemapping results for problematic regions
  updated_finemap_res <- update_finemap_res(finemap_res, finemap_problematic_regions_res)
}
```



## cTWAS output

### Interpreting cTWAS output

We will add the gene names to the results (the PredictDB weights use Ensembl IDs as the primary identifier). 

We then show all molecular traits with PIP > 0.8, which is the threshold we used in the paper.
```{r, eval=FALSE}
# load information for all genes
gene_info <- get_gene_info(weights)

#load information for all genes
gene_info <- data.frame(gene=as.character(), genename=as.character(), gene_type=as.character(), weight=as.character())

for (i in 1:length(weight_files)){
  sqlite <- RSQLite::dbDriver("SQLite")
  db = RSQLite::dbConnect(sqlite, weight_files[i])
  query <- function(...) RSQLite::dbGetQuery(db, ...)
  gene_info_current <- query("select gene, genename, gene_type from extra")
  RSQLite::dbDisconnect(db)

  gene_info_current$weight <- weight_files[i]
  
  gene_info <- rbind(gene_info, gene_info_current)
}

gene_info$weight <- sapply(gene_info$weight, function(x){rev(unlist(strsplit(tools::file_path_sans_ext(x), "/")))[1]})
gene_info$id <- paste(gene_info$gene, gene_info$weight, sep="|")

#add gene names to cTWAS results
ctwas_res$genename[ctwas_res$type!="SNP"] <- gene_info$genename[match(ctwas_res$id[ctwas_res$type!="SNP"], gene_info$id)]

# add gene names to cTWAS results
gene_idx <- match(finemap_res$id[finemap_res$type!="SNP"], gene_info$id)
finemap_res$genename[finemap_res$type!="SNP"] <- gene_info$genename[gene_idx]

# # add z-scores to cTWAS results
# s_idx <- match(finemap_res$id[finemap_res$type=="SNP"], z_snp$id)
# finemap_res$z[finemap_res$type=="SNP"] <- z_snp$z[s_idx]
# 
# g_idx <- match(finemap_res$id[finemap_res$type!="SNP"], z_gene$id)
# finemap_res$z[finemap_res$type!="SNP"] <- z_gene$z[g_idx]

```



### Combining PIPs across tissues or molecular traits

## Visualizing cTWAS results

We could make locus plots for regions of interest.
